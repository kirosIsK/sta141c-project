---
title: "smartLR"
author: "Wing Lo"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{smartLR}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

# Smart Weighed Linear Regression with bootstrap estimation
automatically select Armadillo rewritten method or lm.wfit() method based on dimension of the dataset
  

Introduction
--------
Generate BootStrap Estimation is very time consuming in R.\
This package intends to the speed up the process with the support of parallel Computation.\
Under most of the time, parallel method should be faster than non parallel.\
<br/> <br/>
  
  
  
blblm()
--------

Arguments     | Desciption                                    | Default Value
------------- | --------------------------------------------- | ----------------
formula       | formula for the regression                    | Not Application
data          | dataset for the regression                    | Not Application
m             | num of data to split to                       | 10 
B             | length of Bootstraps for each splited Dataset | 5000
parallel      | True if to use parallel                       | TRUE

<br/>
A remark point here is that blblm() gets linear regression estimates via <br/>
passing matrix and colvec to fastLmXPure() or lm.wfit() which will be faster acutally. \
<br/> <br/>

A packager user, although, is not enforced to tell whether to use parallel method or not,\
which is set to default, however, he is encouraged to use Parallel method.\
Since most function is utilized by Parallel Computing.\
<br/> <br/>



smart_lr() And smart_Pure()
--------
Moreover, Weighted Linear Regression is rewritten using RcppArmadillo.\
Generally, given a dataset with dimension M x N,

Arma Method                       | Lm.wfit Method
--------------------------------- | ------------------------------------
chosen when M < 300 and N < 300   | chosen either M >= 300 and N >= 300

<br/>
Arma method (fastLmX/fastLmXPure) and lm.wfit method <br/>
will be automatically selected based on dimension of the dataset.\
<br/><br/>



Experiment
----------

```{r, echo=FALSE}
install.packages("kernlab",repos = "http://cran.us.r-project.org")
install.packages("tidyverse",repos = "http://cran.us.r-project.org")
install.packages("parallel",repos = "http://cran.us.r-project.org")
install.packages("smartLR",repos = "http://cran.us.r-project.org")
install.packages("rbenchmark",repos = "http://cran.us.r-project.org")
devtools::install_github("ucdavis-sta141c-sq-2020/blblm")
```

Let's Compare the speed with blblm from Package blblm using a large dataset
```{r, warning=FALSE, message=FALSE}
library(kernlab)
library(tidyverse)
library(parallel)
library(blblm)
library(smartLR)
data(spam)
oj <- spam %>% mutate(type = if_else(spam$type == "spam",1,0))
frm    <- formula(log(Volume) ~ log(Girth))

cat(sprintf("Number of observations: %d \nNumber of variables   : %d", nrow(oj), ncol(oj)))

## It seems that Knit somthing can't detect variables inside my function
##  But it doesn't affect the speed of our function

## I am able to run it in RMD without knit
##  Delete tthe below until system.time if you
##  want to try run it by yourfels in RMD
m=1;  B=1; para = TRUE; CL = "CL"
##
## ADD UNtil this  
    
## Acutally Knit stops when they are not aware of the variables
##  Can still execute the function using arguments
##  that being passed to the function

    

system.time({
  # SmartLR -- non parallel method
  smartLR::blblm(type~., data=oj, m=10, B=100, parallel = FALSE)
})

system.time({
  # SmartLR -- parallel method
  smartLR::blblm(type~., data=oj, m=10, B=100, parallel = TRUE)
})

system.time({
  # blblm -- self define method
  blblm::blblm(type~.,   data=oj, m = 10, B = 100)
})

```
As we can see, parallel method is almost 2 times faster than blblm from package blblm.\
<br/>

Let's Compare the speed with blblm from Package blblm using a small dataset \
but with a much larger Bootstraps estimations.
```{r}
library(rbenchmark)
frm <- formula(log(Volume) ~ log(Girth))

cat(sprintf("Number of observations: %d '\n'Number of variables   : %d", nrow(trees), ncol(trees)))


## It seems that Knit somthing can't detect variables inside my function
##  But it doesn't affect the speed of our function

## I am able to run it in RMD without knit
##  Delete tthe below until system.time if you
##  want to try run it by yourfels in RMD
m=1;  B=1; para = TRUE; CL = "CL"
##
## ADD UNtil this  
    
## Acutally Knit stops when they are not aware of the variables
##  Can still execute the function using arguments
##  that being passed to the function


    
# SmartLR -- non parallel method
system.time({
  smartLR::blblm(frm, data=trees, m=10, B=5000, parallel = FALSE)
})

# SmartLR -- parallel method
system.time({
  smartLR::blblm(frm, data=trees, m=10, B=5000, parallel = TRUE)
})

# blblm -- self define method
system.time({
  blblm::blblm(frm,   data=trees, m = 10, B = 5000)
})
```
Actually, parallel method will even have a larger gap with non-parallel method when B and m get larger.\
When B is 100 in the previous example, the strength of parallel computating is not that significant \
because initial resources for allocating clusters is not cheap.\
<br/>

Let's Compare the speed with rewritten weighted linear regression, fastLmX \
with lm() from r-base and fastLm() from RCppArmadillo.
```{r}
library(rbenchmark)
library(RcppArmadillo)

cat(sprintf("Number of observations: %d \nNumber of variables   : %d", nrow(trees), ncol(trees)))
y      <- log(trees$Volume)
X      <- as.matrix(log(trees$Girth))
weight <- replicate(length(y), 1)
frm    <- formula(log(Volume) ~ log(Girth))




benchmark(
  fastLm(frm, data=trees),
  fastLmPure(X, y),
  fastLmX(frm, data=trees, weight),
  fastLmXPure(X, y, weight),
  lm(frm, data=trees, weights = weight)
)
```
We can see fastLmXPure and fastLmPure are the fastest two. This is because they are rewritten \
using Rcpp and Armadillo linear algebra library.\
<br/>

However, when it comes to a larger dataset, \
lm from r-base will eventually have a faster performance than our fastLm and fastLmX codes.
```{r}
library(rsample)
library(kernlab)
library(tidyverse)
library(rbenchmark)
library(RcppArmadillo)
data(spam)

cat(sprintf("Number of observations: %d \nNumber of variables   : %d", nrow(oj), ncol(oj)))
oj      <- spam %>% mutate(type = if_else(spam$type == "spam",1,0))
y_data  <- spam %>% select(type = if_else(spam$type == "spam",1,0)) %>% unlist() %>% as.numeric()
x_data  <- spam %>% select(-type) %>% as.matrix()
Z       <- replicate(length(y_data), 1)


## fast Lm Pure from RcppArmadillo
system.time({
  fastLmPure(x_data, y_data)
})

## Rewritten weighted linear regression using Armadillo
system.time({
  fastLmXPure(x_data, y_data, Z)
})

## lm() in r-base
system.time({
  lm(type~., data=oj)
})
```
We can see that lm is much faster than fastLmPure, fastLmXPure
  when it comes to a high dimension dataset.\
<br/>

But, don't worry. There is function in our package called Smart_lr and Smart_Pure \
that will automatically selected the better method for us.
```{r}
library(rbenchmark)
library(RcppArmadillo)

cat(sprintf("Number of observations: %d \nNumber of variables   : %d", nrow(trees), ncol(trees)))
y      <- log(trees$Volume)
X      <- as.matrix(log(trees$Girth))
weight <- replicate(length(y), 1)
frm    <- formula(log(Volume) ~ log(Girth))



## fast Lm Pure from RcppArmadillo
system.time({
  fastLmPure(X, y)
})

## lm() in r-base
system.time({
  lm(log(trees$Volume)~log(trees$Girth), data=trees, weights = weight)
})

## fastLmX being selected
system.time({
  smart_Pure(X, y, weight)
})
```


```{r}
library(rsample)
library(kernlab)
library(tidyverse)
library(rbenchmark)
library(RcppArmadillo)
data(spam)

cat(sprintf("Number of observations: %d \nNumber of variables   : %d", nrow(oj), ncol(oj)))
oj      <- spam %>% mutate(type = if_else(spam$type == "spam",1,0))
weight  <- replicate(nrow(oj), 1)



## lm
system.time({
  lm(type~., data=oj, weights = weight)
})

## lm.wfit being selected by smart_lr
system.time({
  smart_lr(type~., data=oj, weight = weight)
})
```


