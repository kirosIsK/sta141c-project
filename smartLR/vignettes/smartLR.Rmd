---
title: "smartLR"
author: "Wing Lo"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{smartLR}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

# Smart Linear Regression with bootstrap estimation that automatically 
select Armadillo method or lm.wfit() method based on dimension of the dataset
  

Introduction
--------
- Generate BootStrap Estimation is very time consuming in R. 
  This package intends to the speed up the process with the support of parallel Computation
    Under most of the time, parallel method should be faster than non parallel.
    
  
  Arguments
  - formula  formula for the regression
  - data     dataset for the regression
  - m        num of data to split to
  - B        length of Bootstraps for each splited Dataset
  - parallel True if to use parallel, True by Default
  
  
  A packager user, although, is not enforced to tell whether to use parallel method or not,
    which is set to default, however, he is encouraged to use Parallel method. Since most 
    function is utilized by Parallel Computing.
  
  
  Moreover, Weighted Linear Regression is rewritten using RcppArmadillo. 
    Arma method and lm.wfit method will be automatically selected based on dimension of the dataset
    Generally, given a dataset with dimension M x N,
    Arma Method (fastLmX/fastLmXPure)
      - chosen when M < 300 and N < 300
    lm.wfit Mehod
      - chosen either M >= 300 and N >= 300
  
  

Let's Compare the speed with blblm from Package blblm using a large dataset
```{r, warning=FALSE, message=FALSE}
library(kernlab)
library(tidyverse)
library(parallel)
library(blblm)
library(smartLR)
data(spam)
oj <- spam %>% mutate(type = if_else(spam$type == "spam",1,0))
frm    <- formula(log(Volume) ~ log(Girth))

sprintf("Number of observations: %d", nrow(oj))
sprintf("Number of variables   : %d", ncol(oj))

## It seems that Knit somthing can't detect variables inside my function
##  But it doesn't affect the speed of our function

## I am able to run it in RMD without knit
##  Delete tthe below until system.time if you
##  want to try run it by yourfels in RMD
m=1;  B=1; para = TRUE; CL = "CL"
##
## ADD UNtil this  
    
## Acutally Knit stops when they are not aware of the variables
##  Can still execute the function using arguments
##  that being passed to the function

    

# SmartLR -- parallel method
system.time({
  smartLR::blblm(type~., data=oj, m=10, B=100, parallel = FALSE)
})

# SmartLR -- non parallel method
system.time({
  smartLR::blblm(type~., data=oj, m=10, B=100, parallel = TRUE)
})

# blblm -- self define method
system.time({
  blblm::blblm(type~.,   data=oj, m = 10, B = 100)
})
```
As we can see, parallel method is almost 2 times faster than blblm from package blblm


Let's Compare the speed with blblm from Package blblm using a small dataset
 but with a much larger Bootstraps estimations.
```{r}
library(rbenchmark)
frm <- formula(log(Volume) ~ log(Girth))

sprintf("Number of observations: %d", nrow(trees))
sprintf("Number of variables   : %d", ncol(trees))

## It seems that Knit somthing can't detect variables inside my function
##  But it doesn't affect the speed of our function

## I am able to run it in RMD without knit
##  Delete tthe below until system.time if you
##  want to try run it by yourfels in RMD
m=1;  B=1; para = TRUE; CL = "CL"
##
## ADD UNtil this  
    
## Acutally Knit stops when they are not aware of the variables
##  Can still execute the function using arguments
##  that being passed to the function


    
# SmartLR -- parallel method
system.time({
  smartLR::blblm(frm, data=trees, m=10, B=5000, parallel = FALSE)
})

# SmartLR -- non parallel method
system.time({
  smartLR::blblm(frm, data=trees, m=10, B=5000, parallel = TRUE)
})

# blblm -- self define method
system.time({
  blblm::blblm(frm,   data=trees, m = 10, B = 5000)
})
```
Actually, parallel method will even have a larger gap with non-parallel method
 when B and m get larger. When B is 100 in the previous example, the strength
 of parallel computating is not that significant because initial resources
 for allocating clusters is not cheap.


Let's Compare the speed with rewritten weighted linear regression, fastLmX with
  lm and fastLm from RCppArmadillo.
```{r}
library(rbenchmark)
library(RcppArmadillo)

sprintf("Number of observations: %d", nrow(trees))
sprintf("Number of variables   : %d", ncol(trees))
y      <- log(trees$Volume)
X      <- as.matrix(log(trees$Girth))
weight <- replicate(length(y), 1)
frm    <- formula(log(Volume) ~ log(Girth))




benchmark(
  fastLm(frm, data=trees),
  fastLmPure(X, y),
  fastLmX(frm, data=trees, weight),
  fastLmXPure(X, y, weight),
  lm(frm, data=trees, weights = weight)
)
```
We can see fastLmXPure and fastLmPure are the fastest two. This is because they are rewritten
  using Rcpp and Armadillo linear algebra library.


However, when it comes to a larger dataset, lm from r-base will eventually have a faster
  performance than our fastLm and fastLmX codes.
```{r}
library(rsample)
library(kernlab)
library(tidyverse)
library(rbenchmark)
library(RcppArmadillo)
data(spam)

sprintf("Number of observations: %d", nrow(oj))
sprintf("Number of variables   : %d", ncol(oj))
oj      <- spam %>% mutate(type = if_else(spam$type == "spam",1,0))
y_data  <- spam %>% select(type = if_else(spam$type == "spam",1,0)) %>% unlist() %>% as.numeric()
x_data  <- spam %>% select(-type) %>% as.matrix()
Z       <- replicate(length(y_data), 1)


## fast Lm Pure from RcppArmadillo
system.time({
  fastLmPure(x_data, y_data)
})

## Rewritten weighted linear regression using Armadillo
system.time({
  fastLmXPure(x_data, y_data, Z)
})

## lm() in r-base
system.time({
  lm(type~., data=oj)
})
```
We can see that lm is much faster than fastLmPure, fastLmXPure
  when it comes to a high dimension dataset.


But, don't worry. There is function in our package called 
  Smart_lr and Smart_Pure that will automatically selected the
  better method for us.
```{r}
library(rbenchmark)
library(RcppArmadillo)

sprintf("Number of observations: %d", nrow(trees))
sprintf("Number of variables   : %d", ncol(trees))
y      <- log(trees$Volume)
X      <- as.matrix(log(trees$Girth))
weight <- replicate(length(y), 1)
frm    <- formula(log(Volume) ~ log(Girth))



## fast Lm Pure from RcppArmadillo
system.time({
  fastLmPure(X, y)
})

## lm() in r-base
system.time({
  lm(log(trees$Volume)~log(trees$Girth), data=trees, weights = weight)
})

## fastLmX being selected
system.time({
  smart_Pure(X, y, weight)
})
```


```{r}
library(rsample)
library(kernlab)
library(tidyverse)
library(rbenchmark)
library(RcppArmadillo)
data(spam)

sprintf("Number of observations: %d", nrow(oj))
sprintf("Number of variables   : %d", ncol(oj))
oj      <- spam %>% mutate(type = if_else(spam$type == "spam",1,0))
weight  <- replicate(nrow(oj), 1)



## lm
system.time({
  lm(type~., data=oj, weights = weight)
})

## lm.wfit being selected by smart_lr
system.time({
  smart_lr(type~., data=oj, weight = weight)
})


```


